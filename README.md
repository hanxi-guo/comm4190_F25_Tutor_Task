# Testing LLMs as a Data Cleaning Tutor (in R)

## Overview
This project tests how structured prompting shapes the tutoring behavior of a large language model (LLM).  
The chosen learning task is **Data Cleaning and Summarization in R**.
Two tutoring sessions were run. One unstructured (“vanilla”) and one using the [*Mollick Structured Tutor Prompt*](https://www.moreusefulthings.com/student-exercises) which followed by a modified-prompt experiment.  
All sessions were analyzed to evaluate how prompt structure changes clarity, tone, and engagement in tutoring communication.

---

## File sturcture in this Repository

- [Session1_Vanilla.md](Session1_Vanilla.md)  
  Unstructured tutoring session (no prompt engineering).

- [Session2_Structured.md](Session2_Structured.md)  
  Tutoring session using Mollick’s Structured Tutor Prompt for guided learning.

- [ModifiedPrompt_Test.md](ModifiedPrompt_Test.md)  
  Short follow-up test with a revised version of the structured prompt to improve LLM behavior.

- [Evaluation.md](Evaluation.md)  
  Reflection and comparison of the sessions — includes analysis of wording, communication style, and prompt fine-tuning.

- [README.md](README.md)  
  Current overview of the Tutor Task and file organization.

- [LICENSE](LICENSE)  
  Standard license file for the repository.

- [fine_tuning_command.txt](fine_tuning_command.txt)  
  Additional notes for optional model fine-tuning (not required for evaluation).

---

Author: Hanxi Guo  
Course: COMM 4190 – Talking with AI (University of Pennsylvania)  
Date: October 2025
